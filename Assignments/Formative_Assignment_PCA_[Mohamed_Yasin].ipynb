{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ],
      "metadata": {
        "id": "xyATLU4z1cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary package\n",
        "\n",
        "import numpy as np\n",
        "from scipy import linalg as LA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])"
      ],
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The aim of standardizing the range of the continuous initial variables is so that each one of them contributes equally to the analysis.\n",
        "\n",
        "To be more specific, the reason why it is important to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges. ***For example:*** a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1 which will lead to biased results. So, transforming the data to comparable scales can actually prevent this problem to happen."
      ],
      "metadata": {
        "id": "U2U2_Q5ebos3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(data, axis=0, keepdims=True)\n",
        "\n",
        "standardized_data = (data - mean) / np.std(data, axis=0)\n",
        "\n",
        "# test to see if the data is standardized\n",
        "print(standardized_data)"
      ],
      "metadata": {
        "id": "JF3eGB7FRC0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28368312-6bcc-46f3-c889-808888939e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.36438208  0.70710678  1.5109662  -0.99186978  0.77802924]\n",
            " [ 0.12403473 -1.94454365 -0.13736056  0.77145428 -2.06841919]\n",
            " [-0.62017367  0.1767767   0.68680282 -0.99186978  0.20873955]\n",
            " [ 1.61245155  0.1767767  -1.78568733  0.33062326  0.20873955]\n",
            " [-0.62017367  1.23743687 -0.13736056 -0.77145428  1.00574511]\n",
            " [ 0.86824314 -0.35355339 -0.13736056  1.65311631 -0.13283426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ],
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "uuhux3UEcBgw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "c563be32-18fa-4d02-e9db-6312ac2a6f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.2        -0.42098785 -1.0835838   0.90219291 -0.37000528]\n",
            " [-0.42098785  1.2         0.20397003 -0.77149364  1.18751836]\n",
            " [-1.0835838   0.20397003  1.2        -0.59947269  0.22208218]\n",
            " [ 0.90219291 -0.77149364 -0.59947269  1.2        -0.70017993]\n",
            " [-0.37000528  1.18751836  0.22208218 -0.70017993  1.2       ]]\n"
          ]
        }
      ],
      "source": [
        "# Calculate the covariance matrix\n",
        "cov_matrix = np.cov(standardized_data, rowvar=False)\n",
        "\n",
        "print(cov_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ],
      "metadata": {
        "id": "uXNcG4AFcT08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(\"Eigenvalues: \\n\", eigenvalues) # just checking the data\n",
        "print(\"Eigenvectors: \\n\", eigenvectors)"
      ],
      "metadata": {
        "id": "dmGlQ47tRO5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d977fea-4133-4604-a2cf-2c7d8cb0b920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues: \n",
            " [3.80985761e+00 1.73655615e+00 4.94531029e-02 4.74189469e-05\n",
            " 4.04085720e-01]\n",
            "Eigenvectors: \n",
            " [[-0.4640131   0.45182808 -0.70733581  0.28128049 -0.03317471]\n",
            " [ 0.45019005  0.48800851  0.29051532  0.6706731  -0.15803498]\n",
            " [ 0.37929082 -0.55665017 -0.48462321  0.24186072 -0.5029143 ]\n",
            " [-0.4976889   0.03162214  0.36999674 -0.03373724 -0.78311558]\n",
            " [ 0.43642295  0.49682965 -0.20861365 -0.64143906 -0.32822489]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ],
      "metadata": {
        "id": "4pWho88fcbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]#TO DO: insert code here\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]#TO DO: insert code here]\n",
        "\n",
        "print('\\n\\n sorted eigen values:\\n{}'.format(sorted_eigenvalues))\n",
        "sorted_eigenvectors = eigenvectors[:, order_of_importance]#TO DO: insert code here] # sort the columns\n",
        "print('\\n\\n The sorted eigen vector matrix is: \\n {}'.format(sorted_eigenvectors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "6474df20-9da3-429b-c338-3f26797d0a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the order of importance is :\n",
            " [0 1 4 2 3]\n",
            "\n",
            "\n",
            " sorted eigen values:\n",
            "[3.80985761e+00 1.73655615e+00 4.04085720e-01 4.94531029e-02\n",
            " 4.74189469e-05]\n",
            "\n",
            "\n",
            " The sorted eigen vector matrix is: \n",
            " [[-0.4640131   0.45182808 -0.03317471 -0.70733581  0.28128049]\n",
            " [ 0.45019005  0.48800851 -0.15803498  0.29051532  0.6706731 ]\n",
            " [ 0.37929082 -0.55665017 -0.5029143  -0.48462321  0.24186072]\n",
            " [-0.4976889   0.03162214 -0.78311558  0.36999674 -0.03373724]\n",
            " [ 0.43642295  0.49682965 -0.32822489 -0.20861365 -0.64143906]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "**1. Why do we order eigen values and eigen vectors?**\n",
        "\n",
        "We order eigenvalues and eigenvectors to identify the principal components that capture the most variance in the dataset. This helps us prioritize the most significant dimensions for further analysis or dimensionality reduction.\n",
        "\n",
        "**2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer**\n",
        "\n",
        "\n",
        "No, it is not true. We prioritize the highest eigenvalue over the lowest one. The highest eigenvalue corresponds to the principal component that captures the most variance in the data, making it more important for analysis."
      ],
      "metadata": {
        "id": "o1nILNGxpTJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "BWqFGNeNvgEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "\n",
        "explained_variance = (sorted_eigenvalues / np.sum(sorted_eigenvalues)) * 100\n",
        "explained_variance =[\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "print( explained_variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "8bad5383-0c56-450f-a741-8f07ed2e95bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['63.50%', '28.94%', '6.73%', '0.82%', '0.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ],
      "metadata": {
        "id": "qB7H4InbfKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k =  2# select the number of principal components\n",
        "\n",
        "reduced_data = np.matmul(standardized_data, sorted_eigenvectors[:, :k])"
      ],
      "metadata": {
        "id": "C-Rnyq6QVTiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "3efc7fee-5323-4499-8c8f-03916cae7441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.3577116  -0.75728867]\n",
            " [-2.27171739 -1.81970663]\n",
            " [ 1.21259114 -0.50390931]\n",
            " [-1.41935914  1.9229856 ]\n",
            " [ 1.61562536  0.87541857]\n",
            " [-1.49485157  0.28250044]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "bd64f765-ec15-420d-f8be-eb5f3e6b2745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA?*\n",
        "\n",
        "**Positive Effects (Benefits) of PCA:**\n",
        "\n",
        "A- Dimensionality Reduction: PCA helps in reducing the number of features (dimensions) in a dataset while retaining most of the important information. This reduction in dimensionality leads to simpler models, faster training times, and improved performance.\n",
        "\n",
        "B- Feature Extraction: PCA extracts underlying patterns and relationships between features in high-dimensional data. It identifies the directions (principal components) along which the data varies the most, thereby providing a compact representation of the dataset.\n",
        "\n",
        "**Negative Effects (Limitations) of PCA:**\n",
        "\n",
        "A- Loss of Interpretability: after applying PCA, the original features are transformed into new dimensions (principal components) that may not have direct physical or intuitive meanings. This loss of interpretability can make it challenging to explain the results of the analysis.\n",
        "\n",
        "B- Sensitivity to Outliers: PCA is sensitive to outliers in the data. Outliers can disproportionately influence the covariance matrix and the calculation of principal components, leading to potentially biased results. Preprocessing steps like outlier detection and removal may be necessary to mitigate this issue.\n",
        "\n",
        "\n",
        "                 **© Mohamed Ahmed Yasin**"
      ],
      "metadata": {
        "id": "UxQ8lTunauMQ"
      }
    }
  ]
}